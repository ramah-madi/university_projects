{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e03ce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from builtins import range\n",
    "\n",
    "\"\"\"\n",
    "SECTION 1 : Load and setup data for training\n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data sets\n",
    "IRIS_TRAINING = \"train.txt\"\n",
    "IRIS_TEST = \"test.txt\"\n",
    "\n",
    "train_data = np.genfromtxt(IRIS_TRAINING, skip_header=1, \n",
    "    dtype=float, delimiter=\";\")\n",
    "test_data = np.genfromtxt(IRIS_TEST, skip_header=1, \n",
    "    dtype=float, delimiter=\";\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2477a79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "#split x and y (feature and target)\n",
    "xtrain = train_data[:, :4000]\n",
    "ytrain = train_data[:, 4001]\n",
    "print(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc78c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "SECTION 2 : Build and Train Model\n",
    "\n",
    "Multilayer perceptron model, with one hidden layer.\n",
    "input layer : 4 neuron, represents the feature of Iris\n",
    "hidden layer : 3 neuron, activation using ReLU\n",
    "output layer : 3 neuron, represents the class of Iris\n",
    "\n",
    "optimizer = stochastic gradient descent with no batch-size\n",
    "loss function = categorical cross entropy\n",
    "epoch = 50\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(100)\n",
    "\n",
    "#hyperparameters\n",
    "hl = 2000\n",
    "lr = 0.1\n",
    "num_epoch = 3000\n",
    "\n",
    "#build model\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(4000, hl)\n",
    "        self.fc2 = nn.Linear(hl, hl)\n",
    "        self.fc3 = nn.Linear(hl, hl)\n",
    "        self.fc4 = nn.Linear(hl, hl)\n",
    "        self.fc5 = nn.Linear(hl, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x))\n",
    "        x = F.softmax(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "        \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad0bc5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp/ipykernel_9992/3558169521.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(self.fc2(x))\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp/ipykernel_9992/3558169521.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(self.fc3(x))\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp/ipykernel_9992/3558169521.py:39: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(self.fc4(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3000] Loss: 0.9011   Acc: 89.1921\n",
      "Epoch [52/3000] Loss: 0.3701   Acc: 89.1921\n",
      "Epoch [102/3000] Loss: 0.3479   Acc: 89.1921\n",
      "Epoch [152/3000] Loss: 0.3442   Acc: 89.1921\n",
      "Epoch [202/3000] Loss: 0.3432   Acc: 89.1921\n",
      "Epoch [252/3000] Loss: 0.3428   Acc: 89.1921\n",
      "Epoch [302/3000] Loss: 0.3425   Acc: 89.1921\n",
      "Epoch [352/3000] Loss: 0.3230   Acc: 90.3157\n",
      "Epoch [402/3000] Loss: 0.3167   Acc: 90.4227\n",
      "Epoch [452/3000] Loss: 0.3160   Acc: 90.4227\n",
      "Epoch [502/3000] Loss: 0.3157   Acc: 90.4227\n",
      "Epoch [552/3000] Loss: 0.3156   Acc: 90.4227\n",
      "Epoch [602/3000] Loss: 0.3156   Acc: 90.4227\n",
      "Epoch [652/3000] Loss: 0.3155   Acc: 90.4227\n",
      "Epoch [702/3000] Loss: 0.3155   Acc: 90.4227\n",
      "Epoch [752/3000] Loss: 0.3155   Acc: 90.4227\n",
      "Epoch [802/3000] Loss: 0.3155   Acc: 90.4227\n",
      "Epoch [852/3000] Loss: 0.3155   Acc: 90.4227\n",
      "Epoch [902/3000] Loss: 0.3155   Acc: 90.4227\n",
      "Epoch [952/3000] Loss: 0.3155   Acc: 90.4227\n",
      "Epoch [1002/3000] Loss: 0.3155   Acc: 90.4227\n",
      "Epoch [1052/3000] Loss: 0.3155   Acc: 90.4227\n",
      "Epoch [1102/3000] Loss: 0.3155   Acc: 90.4227\n",
      "Epoch [1152/3000] Loss: 0.3154   Acc: 90.4227\n",
      "Epoch [1202/3000] Loss: 0.3154   Acc: 90.4227\n",
      "Epoch [1252/3000] Loss: 0.3154   Acc: 90.4227\n",
      "Epoch [1302/3000] Loss: 0.3154   Acc: 90.4227\n",
      "Epoch [1352/3000] Loss: 0.3154   Acc: 90.4227\n",
      "Epoch [1402/3000] Loss: 0.3154   Acc: 90.4227\n",
      "Epoch [1452/3000] Loss: 0.3154   Acc: 90.4227\n",
      "Epoch [1502/3000] Loss: 0.3154   Acc: 90.4227\n",
      "Epoch [1552/3000] Loss: 0.3311   Acc: 89.7271\n",
      "Epoch [1602/3000] Loss: 0.3227   Acc: 90.1017\n",
      "Epoch [1652/3000] Loss: 0.3227   Acc: 90.1017\n",
      "Epoch [1702/3000] Loss: 0.3227   Acc: 90.1017\n",
      "Epoch [1752/3000] Loss: 0.3227   Acc: 90.1017\n",
      "Epoch [1802/3000] Loss: 0.3227   Acc: 90.1017\n",
      "Epoch [1852/3000] Loss: 0.3227   Acc: 90.1017\n",
      "Epoch [1902/3000] Loss: 0.3227   Acc: 90.1017\n",
      "Epoch [1952/3000] Loss: 0.3227   Acc: 90.1017\n",
      "Epoch [2002/3000] Loss: 0.3227   Acc: 90.1017\n",
      "Epoch [2052/3000] Loss: 0.3227   Acc: 90.1017\n",
      "Epoch [2102/3000] Loss: 0.3227   Acc: 90.1017\n",
      "Epoch [2152/3000] Loss: 0.3227   Acc: 90.1017\n",
      "Epoch [2202/3000] Loss: 0.3227   Acc: 90.1017\n",
      "Epoch [2252/3000] Loss: 0.3215   Acc: 90.1552\n",
      "Epoch [2302/3000] Loss: 0.3167   Acc: 90.3692\n",
      "Epoch [2352/3000] Loss: 0.3167   Acc: 90.3692\n",
      "Epoch [2402/3000] Loss: 0.3167   Acc: 90.3692\n",
      "Epoch [2452/3000] Loss: 0.3167   Acc: 90.3692\n",
      "Epoch [2502/3000] Loss: 0.3167   Acc: 90.3692\n",
      "Epoch [2552/3000] Loss: 0.3166   Acc: 90.3692\n",
      "Epoch [2602/3000] Loss: 0.3166   Acc: 90.3692\n",
      "Epoch [2652/3000] Loss: 0.3166   Acc: 90.3692\n",
      "Epoch [2702/3000] Loss: 0.3166   Acc: 90.3692\n",
      "Epoch [2752/3000] Loss: 0.3166   Acc: 90.3692\n",
      "Epoch [2802/3000] Loss: 0.3166   Acc: 90.3692\n",
      "Epoch [2852/3000] Loss: 0.3166   Acc: 90.3692\n",
      "Epoch [2902/3000] Loss: 0.3166   Acc: 90.3692\n",
      "Epoch [2952/3000] Loss: 0.3166   Acc: 90.3692\n"
     ]
    }
   ],
   "source": [
    "#choose optimizer and loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "optimizer = torch.optim.Adagrad(net.parameters(), lr=lr)\n",
    "#optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "#optimizer = torch.optim.RMSprop(net.parameters(), lr=lr)\n",
    "\n",
    "#train\n",
    "for epoch in range(num_epoch):\n",
    "    X = torch.from_numpy(xtrain).float()\n",
    "    Y = torch.from_numpy(ytrain).long()\n",
    "\n",
    "    #feedforward - backprop\n",
    "    optimizer.zero_grad()\n",
    "    out = net(X)\n",
    "    loss = criterion(out, Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc = 100 * torch.sum(Y==torch.max(out.data, 1)[1]).double() / len(Y)\n",
    "    if (epoch % 50 == 1):\n",
    "\t    print ('Epoch [%d/%d] Loss: %.4f   Acc: %.4f' \n",
    "                   %(epoch+1, num_epoch, loss.item(), acc.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3547017c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of testing 96.7811 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp/ipykernel_9992/3558169521.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(self.fc2(x))\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp/ipykernel_9992/3558169521.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(self.fc3(x))\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp/ipykernel_9992/3558169521.py:39: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(self.fc4(x))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SECTION 3 : Testing model\n",
    "\"\"\"\n",
    "\n",
    "#split x and y (feature and target)\n",
    "xtest = test_data[:,:4000]\n",
    "ytest = test_data[:,4001]\n",
    "\n",
    "#get prediction\n",
    "X = torch.Tensor(xtest).float()\n",
    "Y = torch.Tensor(ytest).long()\n",
    "out = net(X)\n",
    "_, predicted = torch.max(out.data, 1)\n",
    "\n",
    "#get accuration\n",
    "print('Accuracy of testing %.4f %%' % (100 * torch.sum(Y==predicted).double() / len(Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f86bea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
